25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: -m
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: -r
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: -p
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: --index
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: /work/hg38/chr22.fa
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: -n
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: 32
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: -w
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: ERR000589_1.filt.fastq
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: ERR000589_2.filt.fastq
25/11/06 06:19:26 INFO BwaOptions: [com.github.sparkbwa.BwaOptions] :: Received argument: Output_ERR000589_2219
25/11/06 06:19:26 INFO SparkContext: Running Spark version 3.3.2
25/11/06 06:19:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/11/06 06:19:26 INFO ResourceUtils: ==============================================================
25/11/06 06:19:26 INFO ResourceUtils: No custom resources configured for spark.driver.
25/11/06 06:19:26 INFO ResourceUtils: ==============================================================
25/11/06 06:19:26 INFO SparkContext: Submitted application: SparkBWA_ERR000589_1.filt.fastq-32-NoSort
25/11/06 06:19:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/11/06 06:19:26 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
25/11/06 06:19:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/11/06 06:19:26 INFO SecurityManager: Changing view acls to: root
25/11/06 06:19:26 INFO SecurityManager: Changing modify acls to: root
25/11/06 06:19:26 INFO SecurityManager: Changing view acls groups to: 
25/11/06 06:19:26 INFO SecurityManager: Changing modify acls groups to: 
25/11/06 06:19:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/11/06 06:19:27 INFO Utils: Successfully started service 'sparkDriver' on port 35071.
25/11/06 06:19:27 INFO SparkEnv: Registering MapOutputTracker
25/11/06 06:19:27 INFO SparkEnv: Registering BlockManagerMaster
25/11/06 06:19:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/11/06 06:19:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/11/06 06:19:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/11/06 06:19:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d87886ac-93ff-4587-8ee9-c54840ae88c7
25/11/06 06:19:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/11/06 06:19:27 INFO SparkEnv: Registering OutputCommitCoordinator
25/11/06 06:19:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/11/06 06:19:27 INFO SparkContext: Added JAR file:/work/SparkBWA-jdk8-spark3-v2.jar at spark://spark-client:35071/jars/SparkBWA-jdk8-spark3-v2.jar with timestamp 1762409966560
25/11/06 06:19:27 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at rm/172.20.0.3:8032
25/11/06 06:19:28 INFO Configuration: resource-types.xml not found
25/11/06 06:19:28 INFO ResourceUtils: Unable to find 'resource-types.xml'.
25/11/06 06:19:28 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (32768 MB per container)
25/11/06 06:19:28 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/11/06 06:19:28 INFO Client: Setting up container launch context for our AM
25/11/06 06:19:28 INFO Client: Setting up the launch environment for our AM container
25/11/06 06:19:28 INFO Client: Preparing resources for our AM container
25/11/06 06:19:28 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
25/11/06 06:19:29 INFO Client: Uploading resource file:/tmp/spark-ef3ef1e9-03a7-45f5-9461-9decd2d4a6f0/__spark_libs__9060209297594990311.zip -> hdfs://nn:9000/user/root/.sparkStaging/application_1762401512426_0005/__spark_libs__9060209297594990311.zip
25/11/06 06:19:30 INFO Client: Uploading resource file:/tmp/spark-ef3ef1e9-03a7-45f5-9461-9decd2d4a6f0/__spark_conf__8983899016594306361.zip -> hdfs://nn:9000/user/root/.sparkStaging/application_1762401512426_0005/__spark_conf__.zip
25/11/06 06:19:30 INFO SecurityManager: Changing view acls to: root
25/11/06 06:19:30 INFO SecurityManager: Changing modify acls to: root
25/11/06 06:19:30 INFO SecurityManager: Changing view acls groups to: 
25/11/06 06:19:30 INFO SecurityManager: Changing modify acls groups to: 
25/11/06 06:19:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/11/06 06:19:30 INFO Client: Submitting application application_1762401512426_0005 to ResourceManager
25/11/06 06:19:30 INFO YarnClientImpl: Submitted application application_1762401512426_0005
25/11/06 06:19:31 INFO Client: Application report for application_1762401512426_0005 (state: ACCEPTED)
25/11/06 06:19:31 INFO Client: 
         client token: N/A
         diagnostics: AM container is launched, waiting for AM container to Register with RM
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: default
         start time: 1762409970210
         final status: UNDEFINED
         tracking URL: http://rm:8088/proxy/application_1762401512426_0005/
         user: root
25/11/06 06:19:32 INFO Client: Application report for application_1762401512426_0005 (state: ACCEPTED)
25/11/06 06:19:33 INFO Client: Application report for application_1762401512426_0005 (state: ACCEPTED)
25/11/06 06:19:34 INFO Client: Application report for application_1762401512426_0005 (state: RUNNING)
25/11/06 06:19:34 INFO Client: 
         client token: N/A
         diagnostics: N/A
         ApplicationMaster host: 172.20.0.5
         ApplicationMaster RPC port: -1
         queue: default
         start time: 1762409970210
         final status: UNDEFINED
         tracking URL: http://rm:8088/proxy/application_1762401512426_0005/
         user: root
25/11/06 06:19:34 INFO YarnClientSchedulerBackend: Application application_1762401512426_0005 has started running.
25/11/06 06:19:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37423.
25/11/06 06:19:34 INFO NettyBlockTransferService: Server created on spark-client:37423
25/11/06 06:19:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/11/06 06:19:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-client, 37423, None)
25/11/06 06:19:34 INFO BlockManagerMasterEndpoint: Registering block manager spark-client:37423 with 366.3 MiB RAM, BlockManagerId(driver, spark-client, 37423, None)
25/11/06 06:19:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-client, 37423, None)
25/11/06 06:19:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-client, 37423, None)
25/11/06 06:19:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> rm, PROXY_URI_BASES -> http://rm:8088/proxy/application_1762401512426_0005), /proxy/application_1762401512426_0005
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/06 06:19:34 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/11/06 06:19:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.5:38410) with ID 1,  ResourceProfileId 0
25/11/06 06:19:38 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
25/11/06 06:19:38 INFO BwaInterpreter: [com.github.sparkbwa.BwaInterpreter] :: Starting BWA
25/11/06 06:19:38 INFO BlockManagerMasterEndpoint: Registering block manager nm:37621 with 366.3 MiB RAM, BlockManagerId(1, nm, 37621, None)
25/11/06 06:19:38 INFO BwaInterpreter: [com.github.sparkbwa.BwaInterpreter] ::Not sorting in HDFS. Timing: 16871318958723
25/11/06 06:19:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 361.4 KiB, free 365.9 MiB)
25/11/06 06:19:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.8 KiB, free 365.9 MiB)
25/11/06 06:19:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-client:37423 (size: 32.8 KiB, free: 366.3 MiB)
25/11/06 06:19:38 INFO SparkContext: Created broadcast 0 from textFile at BwaInterpreter.java:149
25/11/06 06:19:39 INFO FileInputFormat: Total input files to process : 1
25/11/06 06:19:39 INFO SparkContext: Starting job: zipWithIndex at BwaInterpreter.java:152
25/11/06 06:19:39 INFO DAGScheduler: Got job 0 (zipWithIndex at BwaInterpreter.java:152) with 13 output partitions
25/11/06 06:19:39 INFO DAGScheduler: Final stage: ResultStage 0 (zipWithIndex at BwaInterpreter.java:152)
25/11/06 06:19:39 INFO DAGScheduler: Parents of final stage: List()
25/11/06 06:19:39 INFO DAGScheduler: Missing parents: List()
25/11/06 06:19:39 INFO DAGScheduler: Submitting ResultStage 0 (ERR000589_1.filt.fastq MapPartitionsRDD[1] at textFile at BwaInterpreter.java:149), which has no missing parents
25/11/06 06:19:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.9 KiB, free 365.9 MiB)
25/11/06 06:19:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 365.9 MiB)
25/11/06 06:19:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-client:37423 (size: 2.8 KiB, free: 366.3 MiB)
25/11/06 06:19:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
25/11/06 06:19:39 INFO DAGScheduler: Submitting 13 missing tasks from ResultStage 0 (ERR000589_1.filt.fastq MapPartitionsRDD[1] at textFile at BwaInterpreter.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
25/11/06 06:19:39 INFO YarnScheduler: Adding task set 0.0 with 13 tasks resource profile 0
25/11/06 06:19:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (nm, executor 1, partition 0, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on nm:37621 (size: 2.8 KiB, free: 366.3 MiB)
25/11/06 06:19:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on nm:37621 (size: 32.8 KiB, free: 366.3 MiB)
25/11/06 06:19:41 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (nm, executor 1, partition 1, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2028 ms on nm (executor 1) (1/13)
25/11/06 06:19:41 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (nm, executor 1, partition 2, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:41 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 670 ms on nm (executor 1) (2/13)
25/11/06 06:19:42 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (nm, executor 1, partition 3, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:42 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 640 ms on nm (executor 1) (3/13)
25/11/06 06:19:43 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (nm, executor 1, partition 4, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:43 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 623 ms on nm (executor 1) (4/13)
25/11/06 06:19:43 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (nm, executor 1, partition 5, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:43 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 619 ms on nm (executor 1) (5/13)
25/11/06 06:19:44 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (nm, executor 1, partition 6, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:44 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 612 ms on nm (executor 1) (6/13)
25/11/06 06:19:45 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (nm, executor 1, partition 7, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:45 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 607 ms on nm (executor 1) (7/13)
25/11/06 06:19:45 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (nm, executor 1, partition 8, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:45 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 610 ms on nm (executor 1) (8/13)
25/11/06 06:19:46 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (nm, executor 1, partition 9, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:46 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 609 ms on nm (executor 1) (9/13)
25/11/06 06:19:46 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (nm, executor 1, partition 10, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:46 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 607 ms on nm (executor 1) (10/13)
25/11/06 06:19:47 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (nm, executor 1, partition 11, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:47 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 614 ms on nm (executor 1) (11/13)
25/11/06 06:19:48 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (nm, executor 1, partition 12, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:48 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 610 ms on nm (executor 1) (12/13)
25/11/06 06:19:48 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 616 ms on nm (executor 1) (13/13)
25/11/06 06:19:48 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/11/06 06:19:48 INFO DAGScheduler: ResultStage 0 (zipWithIndex at BwaInterpreter.java:152) finished in 9.529 s
25/11/06 06:19:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/06 06:19:48 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
25/11/06 06:19:48 INFO DAGScheduler: Job 0 finished: zipWithIndex at BwaInterpreter.java:152, took 9.594905 s
25/11/06 06:19:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 361.4 KiB, free 365.6 MiB)
25/11/06 06:19:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.8 KiB, free 365.5 MiB)
25/11/06 06:19:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-client:37423 (size: 32.8 KiB, free: 366.2 MiB)
25/11/06 06:19:48 INFO SparkContext: Created broadcast 2 from textFile at BwaInterpreter.java:149
25/11/06 06:19:48 INFO FileInputFormat: Total input files to process : 1
25/11/06 06:19:48 INFO SparkContext: Starting job: zipWithIndex at BwaInterpreter.java:152
25/11/06 06:19:48 INFO DAGScheduler: Got job 1 (zipWithIndex at BwaInterpreter.java:152) with 13 output partitions
25/11/06 06:19:48 INFO DAGScheduler: Final stage: ResultStage 1 (zipWithIndex at BwaInterpreter.java:152)
25/11/06 06:19:48 INFO DAGScheduler: Parents of final stage: List()
25/11/06 06:19:48 INFO DAGScheduler: Missing parents: List()
25/11/06 06:19:48 INFO DAGScheduler: Submitting ResultStage 1 (ERR000589_2.filt.fastq MapPartitionsRDD[8] at textFile at BwaInterpreter.java:149), which has no missing parents
25/11/06 06:19:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.9 KiB, free 365.5 MiB)
25/11/06 06:19:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 365.5 MiB)
25/11/06 06:19:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-client:37423 (size: 2.8 KiB, free: 366.2 MiB)
25/11/06 06:19:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
25/11/06 06:19:48 INFO DAGScheduler: Submitting 13 missing tasks from ResultStage 1 (ERR000589_2.filt.fastq MapPartitionsRDD[8] at textFile at BwaInterpreter.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
25/11/06 06:19:48 INFO YarnScheduler: Adding task set 1.0 with 13 tasks resource profile 0
25/11/06 06:19:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 13) (nm, executor 1, partition 0, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on nm:37621 (size: 2.8 KiB, free: 366.3 MiB)
25/11/06 06:19:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on nm:37621 (size: 32.8 KiB, free: 366.2 MiB)
25/11/06 06:19:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 14) (nm, executor 1, partition 1, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 13) in 766 ms on nm (executor 1) (1/13)
25/11/06 06:19:50 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 15) (nm, executor 1, partition 2, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:50 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 14) in 602 ms on nm (executor 1) (2/13)
25/11/06 06:19:50 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 16) (nm, executor 1, partition 3, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:50 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 15) in 611 ms on nm (executor 1) (3/13)
25/11/06 06:19:51 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 17) (nm, executor 1, partition 4, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:51 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 16) in 608 ms on nm (executor 1) (4/13)
25/11/06 06:19:51 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 18) (nm, executor 1, partition 5, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:51 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 17) in 602 ms on nm (executor 1) (5/13)
25/11/06 06:19:52 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 19) (nm, executor 1, partition 6, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:52 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 18) in 604 ms on nm (executor 1) (6/13)
25/11/06 06:19:53 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 20) (nm, executor 1, partition 7, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:53 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 19) in 606 ms on nm (executor 1) (7/13)
25/11/06 06:19:53 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 21) (nm, executor 1, partition 8, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:53 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 20) in 615 ms on nm (executor 1) (8/13)
25/11/06 06:19:54 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 22) (nm, executor 1, partition 9, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:54 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 21) in 611 ms on nm (executor 1) (9/13)
25/11/06 06:19:55 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 23) (nm, executor 1, partition 10, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:55 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 22) in 625 ms on nm (executor 1) (10/13)
25/11/06 06:19:55 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 24) (nm, executor 1, partition 11, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:55 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 23) in 604 ms on nm (executor 1) (11/13)
25/11/06 06:19:56 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 25) (nm, executor 1, partition 12, RACK_LOCAL, 4527 bytes) taskResourceAssignments Map()
25/11/06 06:19:56 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 24) in 600 ms on nm (executor 1) (12/13)
25/11/06 06:19:56 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 25) in 618 ms on nm (executor 1) (13/13)
25/11/06 06:19:56 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/11/06 06:19:56 INFO DAGScheduler: ResultStage 1 (zipWithIndex at BwaInterpreter.java:152) finished in 8.067 s
25/11/06 06:19:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/11/06 06:19:56 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
25/11/06 06:19:56 INFO DAGScheduler: Job 1 finished: zipWithIndex at BwaInterpreter.java:152, took 8.077115 s
25/11/06 06:19:56 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
25/11/06 06:19:56 INFO BlockManager: Removing RDD 6
25/11/06 06:19:56 INFO MapPartitionsRDD: Removing RDD 13 from persistence list
25/11/06 06:19:56 INFO BlockManager: Removing RDD 13
25/11/06 06:19:56 INFO BwaInterpreter: [com.github.sparkbwa.BwaInterpreter] :: No sort with partitioning
25/11/06 06:19:56 INFO BwaInterpreter: [com.github.sparkbwa.BwaInterpreter] :: Repartition with no sort
25/11/06 06:19:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-client:37423 in memory (size: 2.8 KiB, free: 366.2 MiB)
25/11/06 06:19:56 INFO BwaInterpreter: [com.github.sparkbwa.BwaInterpreter] :: End of sorting. Timing: 16889729321464
25/11/06 06:19:56 INFO BwaInterpreter: [com.github.sparkbwa.BwaInterpreter] :: Total time: 0.3068393790166667 minutes
25/11/06 06:19:56 INFO BwaAlignmentBase: [com.github.sparkbwa.BwaPairedAlignment] :: application_1762401512426_0005 - SparkBWA_ERR000589_1.filt.fastq-32-NoSort
25/11/06 06:19:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on nm:37621 in memory (size: 2.8 KiB, free: 366.2 MiB)
25/11/06 06:19:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-client:37423 in memory (size: 2.8 KiB, free: 366.2 MiB)
25/11/06 06:19:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on nm:37621 in memory (size: 2.8 KiB, free: 366.2 MiB)
25/11/06 06:19:56 INFO SparkContext: Starting job: collect at BwaInterpreter.java:305
25/11/06 06:19:56 INFO DAGScheduler: Registering RDD 3 (mapToPair at BwaInterpreter.java:152) as input to shuffle 2
25/11/06 06:19:56 INFO DAGScheduler: Registering RDD 10 (mapToPair at BwaInterpreter.java:152) as input to shuffle 1
25/11/06 06:19:56 INFO DAGScheduler: Registering RDD 17 (repartition at BwaInterpreter.java:281) as input to shuffle 0
25/11/06 06:19:56 INFO DAGScheduler: Got job 2 (collect at BwaInterpreter.java:305) with 32 output partitions
25/11/06 06:19:56 INFO DAGScheduler: Final stage: ResultStage 5 (collect at BwaInterpreter.java:305)
25/11/06 06:19:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
25/11/06 06:19:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
25/11/06 06:19:57 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[3] at mapToPair at BwaInterpreter.java:152), which has no missing parents
25/11/06 06:19:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.4 KiB, free 365.5 MiB)
25/11/06 06:19:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 365.5 MiB)
25/11/06 06:19:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-client:37423 (size: 4.0 KiB, free: 366.2 MiB)
25/11/06 06:19:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513
25/11/06 06:19:57 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[3] at mapToPair at BwaInterpreter.java:152) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))
25/11/06 06:19:57 INFO YarnScheduler: Adding task set 2.0 with 14 tasks resource profile 0
25/11/06 06:19:57 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at mapToPair at BwaInterpreter.java:152), which has no missing parents
25/11/06 06:19:57 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 26) (nm, executor 1, partition 0, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:19:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.4 KiB, free 365.5 MiB)
25/11/06 06:19:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 365.5 MiB)
25/11/06 06:19:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-client:37423 (size: 4.0 KiB, free: 366.2 MiB)
25/11/06 06:19:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
25/11/06 06:19:57 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at mapToPair at BwaInterpreter.java:152) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))
25/11/06 06:19:57 INFO YarnScheduler: Adding task set 3.0 with 14 tasks resource profile 0
25/11/06 06:19:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on nm:37621 (size: 4.0 KiB, free: 366.2 MiB)
25/11/06 06:20:00 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 27) (nm, executor 1, partition 1, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 26) in 3855 ms on nm (executor 1) (1/14)
25/11/06 06:20:04 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 28) (nm, executor 1, partition 2, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:04 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 27) in 3538 ms on nm (executor 1) (2/14)
25/11/06 06:20:07 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 29) (nm, executor 1, partition 3, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:07 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 28) in 3453 ms on nm (executor 1) (3/14)
25/11/06 06:20:11 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 30) (nm, executor 1, partition 4, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:11 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 29) in 3469 ms on nm (executor 1) (4/14)
25/11/06 06:20:14 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 31) (nm, executor 1, partition 5, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:14 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 30) in 3484 ms on nm (executor 1) (5/14)
25/11/06 06:20:18 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 32) (nm, executor 1, partition 6, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:18 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 31) in 3502 ms on nm (executor 1) (6/14)
25/11/06 06:20:21 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 33) (nm, executor 1, partition 7, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:21 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 32) in 3455 ms on nm (executor 1) (7/14)
25/11/06 06:20:25 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 34) (nm, executor 1, partition 8, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:25 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 33) in 3480 ms on nm (executor 1) (8/14)
25/11/06 06:20:28 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 35) (nm, executor 1, partition 9, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:28 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 34) in 3503 ms on nm (executor 1) (9/14)
25/11/06 06:20:32 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 36) (nm, executor 1, partition 10, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:32 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 35) in 3516 ms on nm (executor 1) (10/14)
25/11/06 06:20:35 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 37) (nm, executor 1, partition 11, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:35 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 36) in 3477 ms on nm (executor 1) (11/14)
25/11/06 06:20:39 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 38) (nm, executor 1, partition 12, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:39 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 37) in 3478 ms on nm (executor 1) (12/14)
25/11/06 06:20:42 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 39) (nm, executor 1, partition 13, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:42 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 38) in 3445 ms on nm (executor 1) (13/14)
25/11/06 06:20:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 40) (nm, executor 1, partition 0, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:44 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 39) in 1561 ms on nm (executor 1) (14/14)
25/11/06 06:20:44 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/11/06 06:20:44 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at BwaInterpreter.java:152) finished in 47.214 s
25/11/06 06:20:44 INFO DAGScheduler: looking for newly runnable stages
25/11/06 06:20:44 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
25/11/06 06:20:44 INFO DAGScheduler: waiting: Set(ResultStage 5, ShuffleMapStage 4)
25/11/06 06:20:44 INFO DAGScheduler: failed: Set()
25/11/06 06:20:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on nm:37621 (size: 4.0 KiB, free: 366.2 MiB)
25/11/06 06:20:47 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 41) (nm, executor 1, partition 1, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 40) in 3617 ms on nm (executor 1) (1/14)
25/11/06 06:20:51 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 42) (nm, executor 1, partition 2, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:51 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 41) in 3500 ms on nm (executor 1) (2/14)
25/11/06 06:20:54 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 43) (nm, executor 1, partition 3, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:54 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 42) in 3483 ms on nm (executor 1) (3/14)
25/11/06 06:20:58 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 44) (nm, executor 1, partition 4, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:20:58 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 43) in 3460 ms on nm (executor 1) (4/14)
25/11/06 06:21:01 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 45) (nm, executor 1, partition 5, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:01 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 44) in 3494 ms on nm (executor 1) (5/14)
25/11/06 06:21:05 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 46) (nm, executor 1, partition 6, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:05 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 45) in 3488 ms on nm (executor 1) (6/14)
25/11/06 06:21:08 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 47) (nm, executor 1, partition 7, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:08 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 46) in 3457 ms on nm (executor 1) (7/14)
25/11/06 06:21:12 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 48) (nm, executor 1, partition 8, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:12 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 47) in 3489 ms on nm (executor 1) (8/14)
25/11/06 06:21:15 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 49) (nm, executor 1, partition 9, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:15 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 48) in 3498 ms on nm (executor 1) (9/14)
25/11/06 06:21:19 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 50) (nm, executor 1, partition 10, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:19 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 49) in 3510 ms on nm (executor 1) (10/14)
25/11/06 06:21:22 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 51) (nm, executor 1, partition 11, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:22 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 50) in 3525 ms on nm (executor 1) (11/14)
25/11/06 06:21:26 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 52) (nm, executor 1, partition 12, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:26 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 51) in 3470 ms on nm (executor 1) (12/14)
25/11/06 06:21:29 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 53) (nm, executor 1, partition 13, RACK_LOCAL, 4626 bytes) taskResourceAssignments Map()
25/11/06 06:21:29 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 52) in 3461 ms on nm (executor 1) (13/14)
25/11/06 06:21:31 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 53) in 1552 ms on nm (executor 1) (14/14)
25/11/06 06:21:31 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
25/11/06 06:21:31 INFO DAGScheduler: ShuffleMapStage 3 (mapToPair at BwaInterpreter.java:152) finished in 94.171 s
25/11/06 06:21:31 INFO DAGScheduler: looking for newly runnable stages
25/11/06 06:21:31 INFO DAGScheduler: running: Set()
25/11/06 06:21:31 INFO DAGScheduler: waiting: Set(ResultStage 5, ShuffleMapStage 4)
25/11/06 06:21:31 INFO DAGScheduler: failed: Set()
25/11/06 06:21:31 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at BwaInterpreter.java:281), which has no missing parents
25/11/06 06:21:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.2 KiB, free 365.5 MiB)
25/11/06 06:21:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 365.5 MiB)
25/11/06 06:21:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-client:37423 (size: 5.4 KiB, free: 366.2 MiB)
25/11/06 06:21:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513
25/11/06 06:21:31 INFO DAGScheduler: Submitting 14 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[17] at repartition at BwaInterpreter.java:281) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))
25/11/06 06:21:31 INFO YarnScheduler: Adding task set 4.0 with 14 tasks resource profile 0
25/11/06 06:21:31 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 54) (nm, executor 1, partition 0, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:21:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on nm:37621 (size: 5.4 KiB, free: 366.2 MiB)
25/11/06 06:21:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.20.0.5:38410
25/11/06 06:21:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.20.0.5:38410
25/11/06 06:22:05 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 55) (nm, executor 1, partition 1, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:22:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 54) in 33861 ms on nm (executor 1) (1/14)
25/11/06 06:22:37 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 56) (nm, executor 1, partition 2, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:22:37 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 55) in 32720 ms on nm (executor 1) (2/14)
25/11/06 06:23:13 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 57) (nm, executor 1, partition 3, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:23:13 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 56) in 36133 ms on nm (executor 1) (3/14)
25/11/06 06:23:48 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 58) (nm, executor 1, partition 4, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:23:48 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 57) in 34507 ms on nm (executor 1) (4/14)
25/11/06 06:24:22 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 59) (nm, executor 1, partition 5, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:24:22 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 58) in 34470 ms on nm (executor 1) (5/14)
25/11/06 06:24:54 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 60) (nm, executor 1, partition 6, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:24:54 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 59) in 31702 ms on nm (executor 1) (6/14)
25/11/06 06:25:29 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 61) (nm, executor 1, partition 7, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:25:29 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 60) in 34604 ms on nm (executor 1) (7/14)
25/11/06 06:26:01 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 62) (nm, executor 1, partition 8, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:26:01 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 61) in 32663 ms on nm (executor 1) (8/14)
25/11/06 06:26:33 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 63) (nm, executor 1, partition 9, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:26:33 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 62) in 31362 ms on nm (executor 1) (9/14)
25/11/06 06:27:07 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 64) (nm, executor 1, partition 10, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:27:07 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 63) in 34025 ms on nm (executor 1) (10/14)
25/11/06 06:27:39 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 65) (nm, executor 1, partition 11, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:27:39 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 64) in 32435 ms on nm (executor 1) (11/14)
25/11/06 06:28:13 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 66) (nm, executor 1, partition 12, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:28:13 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 65) in 34171 ms on nm (executor 1) (12/14)
25/11/06 06:28:48 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 67) (nm, executor 1, partition 13, NODE_LOCAL, 4510 bytes) taskResourceAssignments Map()
25/11/06 06:28:48 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 66) in 34428 ms on nm (executor 1) (13/14)
25/11/06 06:29:24 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 67) in 35713 ms on nm (executor 1) (14/14)
25/11/06 06:29:24 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/11/06 06:29:24 INFO DAGScheduler: ShuffleMapStage 4 (repartition at BwaInterpreter.java:281) finished in 472.800 s
25/11/06 06:29:24 INFO DAGScheduler: looking for newly runnable stages
25/11/06 06:29:24 INFO DAGScheduler: running: Set()
25/11/06 06:29:24 INFO DAGScheduler: waiting: Set(ResultStage 5)
25/11/06 06:29:24 INFO DAGScheduler: failed: Set()
25/11/06 06:29:24 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at mapPartitionsWithIndex at BwaInterpreter.java:304), which has no missing parents
25/11/06 06:29:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.5 KiB, free 365.5 MiB)
25/11/06 06:29:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 365.5 MiB)
25/11/06 06:29:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-client:37423 (size: 4.0 KiB, free: 366.2 MiB)
25/11/06 06:29:24 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
25/11/06 06:29:24 INFO DAGScheduler: Submitting 32 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at mapPartitionsWithIndex at BwaInterpreter.java:304) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
25/11/06 06:29:24 INFO YarnScheduler: Adding task set 5.0 with 32 tasks resource profile 0
25/11/06 06:29:24 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 68) (nm, executor 1, partition 0, NODE_LOCAL, 4558 bytes) taskResourceAssignments Map()
25/11/06 06:29:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on nm:37621 (size: 4.0 KiB, free: 366.2 MiB)
25/11/06 06:29:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.5:38410
25/11/06 06:31:15 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 69) (nm, executor 1, partition 1, NODE_LOCAL, 4558 bytes) taskResourceAssignments Map()
25/11/06 06:31:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 68) in 111879 ms on nm (executor 1) (1/32)
25/11/06 06:33:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 1.
25/11/06 06:33:16 INFO DAGScheduler: Executor lost: 1 (epoch 4)
25/11/06 06:33:16 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
25/11/06 06:33:16 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, nm, 37621, None)
25/11/06 06:33:16 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
25/11/06 06:33:16 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 4)
25/11/06 06:33:16 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1762401512426_0005_01_000002 on host: nm. Exit status: 134. Diagnostics: rr :
 for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 227, 246)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (147, 312)
[M::mem_pestat] mean and std.dev: (227.75, 20.65)
[M::mem_pestat] low and high boundaries for proper pairs: (114, 345)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (97, 126, 366)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 904)
[M::mem_pestat] mean and std.dev: (184.72, 137.84)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1173)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 30.614 CPU sec, 30.518 real sec
[M::process] read 196080 sequences (10000080 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1145, 97, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 229, 248)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (143, 318)
[M::mem_pestat] mean and std.dev: (229.27, 20.98)
[M::mem_pestat] low and high boundaries for proper pairs: (108, 353)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (105, 132, 380)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 930)
[M::mem_pestat] mean and std.dev: (187.08, 122.44)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1205)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.000 CPU sec, 30.855 real sec
[M::process] read 159516 sequences (8135316 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1154, 102, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (215, 231, 247)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (151, 311)
[M::mem_pestat] mean and std.dev: (229.73, 19.34)
[M::mem_pestat] low and high boundaries for proper pairs: (119, 343)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (102, 132, 390)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 966)
[M::mem_pestat] mean and std.dev: (232.67, 172.74)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1254)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.597 CPU sec, 31.465 real sec
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 958, 77, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 230, 245)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (149, 309)
[M::mem_pestat] mean and std.dev: (228.22, 20.22)
[M::mem_pestat] low and high boundaries for proper pairs: (117, 341)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (96, 117, 369)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 915)
[M::mem_pestat] mean and std.dev: (181.32, 129.16)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1188)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 159516 reads in 25.257 CPU sec, 25.188 real sec
[main] Version: 0.7.15-r1140
[main] CMD: bwa mem -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589 /work/hg38/chr22.fa /tmp/application_1762401512426_0005-RDD1_1 /tmp/application_1762401512426_0005-RDD1_2
[main] Real time: 118.188 sec; CPU: 1185.378 sec
[Java_com_github_sparkbwa_BwaJni_bwa_1jni] Return code from BWA 0.
free(): double free detected in tcache 2


[2025-11-06 06:33:16.668]Container exited with a non-zero exit code 134. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
/bin/bash: line 1:  1224 Aborted                 (core dumped) /opt/java/openjdk/bin/java -server -Xmx1024m '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED' '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '--add-opens=java.base/java.net=ALL-UNNAMED' '--add-opens=java.base/java.nio=ALL-UNNAMED' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED' '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED' '--add-opens=java.base/sun.security.action=ALL-UNNAMED' '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED' '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED' -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1762401512426_0005/container_1762401512426_0005_01_000002/tmp '-Dspark.driver.port=35071' -Dspark.yarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@spark-client:35071 --executor-id 1 --hostname nm --cores 1 --app-id application_1762401512426_0005 --resourceProfileId 0 > /opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002/stdout 2> /opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002/stderr
Last 4096 bytes of stderr :
 for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 227, 246)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (147, 312)
[M::mem_pestat] mean and std.dev: (227.75, 20.65)
[M::mem_pestat] low and high boundaries for proper pairs: (114, 345)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (97, 126, 366)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 904)
[M::mem_pestat] mean and std.dev: (184.72, 137.84)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1173)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 30.614 CPU sec, 30.518 real sec
[M::process] read 196080 sequences (10000080 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1145, 97, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 229, 248)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (143, 318)
[M::mem_pestat] mean and std.dev: (229.27, 20.98)
[M::mem_pestat] low and high boundaries for proper pairs: (108, 353)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (105, 132, 380)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 930)
[M::mem_pestat] mean and std.dev: (187.08, 122.44)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1205)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.000 CPU sec, 30.855 real sec
[M::process] read 159516 sequences (8135316 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1154, 102, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (215, 231, 247)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (151, 311)
[M::mem_pestat] mean and std.dev: (229.73, 19.34)
[M::mem_pestat] low and high boundaries for proper pairs: (119, 343)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (102, 132, 390)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 966)
[M::mem_pestat] mean and std.dev: (232.67, 172.74)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1254)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.597 CPU sec, 31.465 real sec
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 958, 77, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 230, 245)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (149, 309)
[M::mem_pestat] mean and std.dev: (228.22, 20.22)
[M::mem_pestat] low and high boundaries for proper pairs: (117, 341)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (96, 117, 369)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 915)
[M::mem_pestat] mean and std.dev: (181.32, 129.16)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1188)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 159516 reads in 25.257 CPU sec, 25.188 real sec
[main] Version: 0.7.15-r1140
[main] CMD: bwa mem -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589 /work/hg38/chr22.fa /tmp/application_1762401512426_0005-RDD1_1 /tmp/application_1762401512426_0005-RDD1_2
[main] Real time: 118.188 sec; CPU: 1185.378 sec
[Java_com_github_sparkbwa_BwaJni_bwa_1jni] Return code from BWA 0.
free(): double free detected in tcache 2


.
25/11/06 06:33:16 ERROR YarnScheduler: Lost executor 1 on nm: Container from a bad node: container_1762401512426_0005_01_000002 on host: nm. Exit status: 134. Diagnostics: rr :
 for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 227, 246)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (147, 312)
[M::mem_pestat] mean and std.dev: (227.75, 20.65)
[M::mem_pestat] low and high boundaries for proper pairs: (114, 345)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (97, 126, 366)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 904)
[M::mem_pestat] mean and std.dev: (184.72, 137.84)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1173)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 30.614 CPU sec, 30.518 real sec
[M::process] read 196080 sequences (10000080 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1145, 97, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 229, 248)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (143, 318)
[M::mem_pestat] mean and std.dev: (229.27, 20.98)
[M::mem_pestat] low and high boundaries for proper pairs: (108, 353)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (105, 132, 380)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 930)
[M::mem_pestat] mean and std.dev: (187.08, 122.44)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1205)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.000 CPU sec, 30.855 real sec
[M::process] read 159516 sequences (8135316 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1154, 102, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (215, 231, 247)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (151, 311)
[M::mem_pestat] mean and std.dev: (229.73, 19.34)
[M::mem_pestat] low and high boundaries for proper pairs: (119, 343)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (102, 132, 390)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 966)
[M::mem_pestat] mean and std.dev: (232.67, 172.74)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1254)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.597 CPU sec, 31.465 real sec
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 958, 77, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 230, 245)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (149, 309)
[M::mem_pestat] mean and std.dev: (228.22, 20.22)
[M::mem_pestat] low and high boundaries for proper pairs: (117, 341)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (96, 117, 369)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 915)
[M::mem_pestat] mean and std.dev: (181.32, 129.16)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1188)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 159516 reads in 25.257 CPU sec, 25.188 real sec
[main] Version: 0.7.15-r1140
[main] CMD: bwa mem -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589 /work/hg38/chr22.fa /tmp/application_1762401512426_0005-RDD1_1 /tmp/application_1762401512426_0005-RDD1_2
[main] Real time: 118.188 sec; CPU: 1185.378 sec
[Java_com_github_sparkbwa_BwaJni_bwa_1jni] Return code from BWA 0.
free(): double free detected in tcache 2


[2025-11-06 06:33:16.668]Container exited with a non-zero exit code 134. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
/bin/bash: line 1:  1224 Aborted                 (core dumped) /opt/java/openjdk/bin/java -server -Xmx1024m '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED' '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '--add-opens=java.base/java.net=ALL-UNNAMED' '--add-opens=java.base/java.nio=ALL-UNNAMED' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED' '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED' '--add-opens=java.base/sun.security.action=ALL-UNNAMED' '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED' '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED' -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1762401512426_0005/container_1762401512426_0005_01_000002/tmp '-Dspark.driver.port=35071' -Dspark.yarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@spark-client:35071 --executor-id 1 --hostname nm --cores 1 --app-id application_1762401512426_0005 --resourceProfileId 0 > /opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002/stdout 2> /opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002/stderr
Last 4096 bytes of stderr :
 for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 227, 246)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (147, 312)
[M::mem_pestat] mean and std.dev: (227.75, 20.65)
[M::mem_pestat] low and high boundaries for proper pairs: (114, 345)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (97, 126, 366)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 904)
[M::mem_pestat] mean and std.dev: (184.72, 137.84)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1173)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 30.614 CPU sec, 30.518 real sec
[M::process] read 196080 sequences (10000080 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1145, 97, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 229, 248)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (143, 318)
[M::mem_pestat] mean and std.dev: (229.27, 20.98)
[M::mem_pestat] low and high boundaries for proper pairs: (108, 353)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (105, 132, 380)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 930)
[M::mem_pestat] mean and std.dev: (187.08, 122.44)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1205)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.000 CPU sec, 30.855 real sec
[M::process] read 159516 sequences (8135316 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1154, 102, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (215, 231, 247)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (151, 311)
[M::mem_pestat] mean and std.dev: (229.73, 19.34)
[M::mem_pestat] low and high boundaries for proper pairs: (119, 343)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (102, 132, 390)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 966)
[M::mem_pestat] mean and std.dev: (232.67, 172.74)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1254)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.597 CPU sec, 31.465 real sec
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 958, 77, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 230, 245)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (149, 309)
[M::mem_pestat] mean and std.dev: (228.22, 20.22)
[M::mem_pestat] low and high boundaries for proper pairs: (117, 341)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (96, 117, 369)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 915)
[M::mem_pestat] mean and std.dev: (181.32, 129.16)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1188)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 159516 reads in 25.257 CPU sec, 25.188 real sec
[main] Version: 0.7.15-r1140
[main] CMD: bwa mem -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589 /work/hg38/chr22.fa /tmp/application_1762401512426_0005-RDD1_1 /tmp/application_1762401512426_0005-RDD1_2
[main] Real time: 118.188 sec; CPU: 1185.378 sec
[Java_com_github_sparkbwa_BwaJni_bwa_1jni] Return code from BWA 0.
free(): double free detected in tcache 2


.
25/11/06 06:33:16 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 69) (nm executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1762401512426_0005_01_000002 on host: nm. Exit status: 134. Diagnostics: rr :
 for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 227, 246)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (147, 312)
[M::mem_pestat] mean and std.dev: (227.75, 20.65)
[M::mem_pestat] low and high boundaries for proper pairs: (114, 345)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (97, 126, 366)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 904)
[M::mem_pestat] mean and std.dev: (184.72, 137.84)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1173)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 30.614 CPU sec, 30.518 real sec
[M::process] read 196080 sequences (10000080 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1145, 97, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 229, 248)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (143, 318)
[M::mem_pestat] mean and std.dev: (229.27, 20.98)
[M::mem_pestat] low and high boundaries for proper pairs: (108, 353)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (105, 132, 380)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 930)
[M::mem_pestat] mean and std.dev: (187.08, 122.44)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1205)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.000 CPU sec, 30.855 real sec
[M::process] read 159516 sequences (8135316 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1154, 102, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (215, 231, 247)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (151, 311)
[M::mem_pestat] mean and std.dev: (229.73, 19.34)
[M::mem_pestat] low and high boundaries for proper pairs: (119, 343)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (102, 132, 390)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 966)
[M::mem_pestat] mean and std.dev: (232.67, 172.74)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1254)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.597 CPU sec, 31.465 real sec
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 958, 77, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 230, 245)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (149, 309)
[M::mem_pestat] mean and std.dev: (228.22, 20.22)
[M::mem_pestat] low and high boundaries for proper pairs: (117, 341)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (96, 117, 369)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 915)
[M::mem_pestat] mean and std.dev: (181.32, 129.16)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1188)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 159516 reads in 25.257 CPU sec, 25.188 real sec
[main] Version: 0.7.15-r1140
[main] CMD: bwa mem -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589 /work/hg38/chr22.fa /tmp/application_1762401512426_0005-RDD1_1 /tmp/application_1762401512426_0005-RDD1_2
[main] Real time: 118.188 sec; CPU: 1185.378 sec
[Java_com_github_sparkbwa_BwaJni_bwa_1jni] Return code from BWA 0.
free(): double free detected in tcache 2


[2025-11-06 06:33:16.668]Container exited with a non-zero exit code 134. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
/bin/bash: line 1:  1224 Aborted                 (core dumped) /opt/java/openjdk/bin/java -server -Xmx1024m '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED' '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '--add-opens=java.base/java.net=ALL-UNNAMED' '--add-opens=java.base/java.nio=ALL-UNNAMED' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED' '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED' '--add-opens=java.base/sun.security.action=ALL-UNNAMED' '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED' '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED' -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1762401512426_0005/container_1762401512426_0005_01_000002/tmp '-Dspark.driver.port=35071' -Dspark.yarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@spark-client:35071 --executor-id 1 --hostname nm --cores 1 --app-id application_1762401512426_0005 --resourceProfileId 0 > /opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002/stdout 2> /opt/hadoop/logs/userlogs/application_1762401512426_0005/container_1762401512426_0005_01_000002/stderr
Last 4096 bytes of stderr :
 for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 227, 246)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (147, 312)
[M::mem_pestat] mean and std.dev: (227.75, 20.65)
[M::mem_pestat] low and high boundaries for proper pairs: (114, 345)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (97, 126, 366)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 904)
[M::mem_pestat] mean and std.dev: (184.72, 137.84)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1173)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 30.614 CPU sec, 30.518 real sec
[M::process] read 196080 sequences (10000080 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1145, 97, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 229, 248)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (143, 318)
[M::mem_pestat] mean and std.dev: (229.27, 20.98)
[M::mem_pestat] low and high boundaries for proper pairs: (108, 353)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (105, 132, 380)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 930)
[M::mem_pestat] mean and std.dev: (187.08, 122.44)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1205)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.000 CPU sec, 30.855 real sec
[M::process] read 159516 sequences (8135316 bp)...
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 1154, 102, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (215, 231, 247)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (151, 311)
[M::mem_pestat] mean and std.dev: (229.73, 19.34)
[M::mem_pestat] low and high boundaries for proper pairs: (119, 343)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (102, 132, 390)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 966)
[M::mem_pestat] mean and std.dev: (232.67, 172.74)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1254)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 196080 reads in 31.597 CPU sec, 31.465 real sec
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (0, 958, 77, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (213, 230, 245)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (149, 309)
[M::mem_pestat] mean and std.dev: (228.22, 20.22)
[M::mem_pestat] low and high boundaries for proper pairs: (117, 341)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (96, 117, 369)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 915)
[M::mem_pestat] mean and std.dev: (181.32, 129.16)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1188)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_process_seqs] Processed 159516 reads in 25.257 CPU sec, 25.188 real sec
[main] Version: 0.7.15-r1140
[main] CMD: bwa mem -R @RG\tID:foo\tLB:bar\tPL:illumina\tPU:illumina\tSM:ERR000589 /work/hg38/chr22.fa /tmp/application_1762401512426_0005-RDD1_1 /tmp/application_1762401512426_0005-RDD1_2
[main] Real time: 118.188 sec; CPU: 1185.378 sec
[Java_com_github_sparkbwa_BwaJni_bwa_1jni] Return code from BWA 0.
free(): double free detected in tcache 2


.
25/11/06 06:33:16 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
25/11/06 06:33:16 INFO BlockManagerMaster: Removal of executor 1 requested
25/11/06 06:33:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 1
25/11/06 06:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.5:38416) with ID 2,  ResourceProfileId 0
25/11/06 06:33:23 INFO BlockManagerMasterEndpoint: Registering block manager nm:44147 with 366.3 MiB RAM, BlockManagerId(2, nm, 44147, None)
25/11/06 06:33:23 INFO TaskSetManager: Starting task 1.1 in stage 5.0 (TID 70) (nm, executor 2, partition 1, NODE_LOCAL, 4558 bytes) taskResourceAssignments Map()
25/11/06 06:33:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on nm:44147 (size: 4.0 KiB, free: 366.3 MiB)
25/11/06 06:33:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.5:38416
25/11/06 06:33:24 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 71) (nm, executor 2, partition 2, NODE_LOCAL, 4558 bytes) taskResourceAssignments Map()
25/11/06 06:33:24 WARN TaskSetManager: Lost task 1.1 in stage 5.0 (TID 70) (nm executor 2): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 1
        at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)
        at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)
        at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)
        at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)
        at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)
        at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
        at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
        at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
        at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)
        at com.github.sparkbwa.BwaPairedAlignment.call(BwaPairedAlignment.java:94)
        at com.github.sparkbwa.BwaPairedAlignment.call(BwaPairedAlignment.java:33)
        at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)
        at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:907)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:907)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)

)
25/11/06 06:33:24 INFO TaskSetManager: task 1.1 in stage 5.0 (TID 70) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
25/11/06 06:33:24 INFO DAGScheduler: Marking ResultStage 5 (collect at BwaInterpreter.java:305) as failed due to a fetch failure from ShuffleMapStage 4 (repartition at BwaInterpreter.java:281)
25/11/06 06:33:24 INFO DAGScheduler: ResultStage 5 (collect at BwaInterpreter.java:305) failed in 240.422 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 1
        at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)
        at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)
        at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)
        at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)
        at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)
        at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
        at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
        at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
        at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)
        at com.github.sparkbwa.BwaPairedAlignment.call(BwaPairedAlignment.java:94)
        at com.github.sparkbwa.BwaPairedAlignment.call(BwaPairedAlignment.java:33)
        at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)
        at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:907)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:907)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)

25/11/06 06:33:24 INFO DAGScheduler: Job 2 failed: collect at BwaInterpreter.java:305, took 807.469506 s
25/11/06 06:33:24 INFO DAGScheduler: The shuffle map stage ShuffleMapStage 4 with indeterminate output was failed, we will roll back and rerun below stages which include itself and all its indeterminate child stages: Set(ShuffleMapStage 4)
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: A shuffle map stage with indeterminate output was failed and retried. However, Spark cannot rollback the ResultStage 5 to re-process the input data, and has to fail this job. Please eliminate the indeterminacy by checkpointing the RDD before repartition and try again.
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskCompletion$12(DAGScheduler.scala:1978)
        at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1958)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2857)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
        at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)
        at org.apache.spark.api.java.JavaRDDLike.collect(JavaRDDLike.scala:362)
        at org.apache.spark.api.java.JavaRDDLike.collect$(JavaRDDLike.scala:361)
        at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
        at com.github.sparkbwa.BwaInterpreter.MapPairedBwa(BwaInterpreter.java:305)
        at com.github.sparkbwa.BwaInterpreter.runBwa(BwaInterpreter.java:334)
        at com.github.sparkbwa.SparkBWA.main(SparkBWA.java:37)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
25/11/06 06:33:24 INFO DAGScheduler: Resubmitting ShuffleMapStage 4 (repartition at BwaInterpreter.java:281) and ResultStage 5 (collect at BwaInterpreter.java:305) due to fetch failure
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/11/06 06:33:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.5:38416
25/11/06 06:33:24 INFO SparkContext: Invoking stop() from shutdown hook
25/11/06 06:33:24 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 71) (nm executor 2): FetchFailed(null, shuffleId=0, mapIndex=-1, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 partition 2
        at org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)
        at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)
        at org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)
        at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)
        at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)
        at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)
        at org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)
        at org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)
        at org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)
        at com.github.sparkbwa.BwaPairedAlignment.call(BwaPairedAlignment.java:94)
        at com.github.sparkbwa.BwaPairedAlignment.call(BwaPairedAlignment.java:33)
        at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)
        at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:907)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:907)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)

)
25/11/06 06:33:24 INFO TaskSetManager: task 2.0 in stage 5.0 (TID 71) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
25/11/06 06:33:24 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
25/11/06 06:33:24 INFO SparkUI: Stopped Spark web UI at http://spark-client:4040
25/11/06 06:33:24 INFO YarnClientSchedulerBackend: Interrupting monitor thread
25/11/06 06:33:24 INFO YarnClientSchedulerBackend: Shutting down all executors
25/11/06 06:33:24 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/11/06 06:33:24 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
25/11/06 06:33:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/11/06 06:33:24 INFO MemoryStore: MemoryStore cleared
25/11/06 06:33:24 INFO BlockManager: BlockManager stopped
25/11/06 06:33:24 INFO BlockManagerMaster: BlockManagerMaster stopped
25/11/06 06:33:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/11/06 06:33:24 INFO SparkContext: Successfully stopped SparkContext
25/11/06 06:33:24 INFO ShutdownHookManager: Shutdown hook called
25/11/06 06:33:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ef3ef1e9-03a7-45f5-9461-9decd2d4a6f0
25/11/06 06:33:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-29c053bd-32f8-4fd5-9894-569b99429802
root@spark-client:/work# ls

